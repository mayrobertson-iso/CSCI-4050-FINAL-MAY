# -*- coding: utf-8 -*-
"""CSCI FINAL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QSJ7Q1lcpnzY1tJDyZ7_hZ4C4GxCuZTw
"""

! pip install lightning > /dev/null
! pip install tokenizers > /dev/null

import pandas as pd
import time

import time
import json
import pandas as pd
import pandas as pd

# only first 50000 rows
n_rows = 50000
df = pd.read_json("900k Definitive Spotify Dataset.json", lines=True, nrows=n_rows)

# only necessary columns
df = df[['song', 'emotion', 'text']]

print(f"Loaded {len(df)} rows")
print(df.head())

df["emotion"].value_counts()

# remove unecessary emotions

df=df[df["emotion"] != "True"]
df=df[df["emotion"] != "angry"]
df=df[df["emotion"] != "surprise"]
df=df[df["emotion"] != "love"]
df=df[df["emotion"] != "fear"]
df.dropna()

print(df['emotion'].value_counts())



from tokenizers import Tokenizer
from tokenizers.models import WordLevel, WordPiece
from tokenizers.trainers import WordLevelTrainer, WordPieceTrainer
from tokenizers.pre_tokenizers import Whitespace
from pytorch_lightning.callbacks.early_stopping import EarlyStopping

def balance_cats(df):
  # make sure we have same number of entries for each emotion
  targets = {
      'joy': 5000,
      'sadness': 5000,
      'anger': 5000,
  }

  balanced_dfs = []
  for emotion, target in targets.items():
      emotion_df = df[df['emotion'] == emotion]
      balanced_dfs.append(emotion_df.sample(target, random_state=42))


  return pd.concat(balanced_dfs, ignore_index=True)

df_b = balance_cats(df)
print("Final balanced dataset:")
print(df_b['emotion'].value_counts())
print(f"Total samples: {len(df_b)}")

print(df_b.head())
print(df_b['emotion'].value_counts())

# save to csv for easy access
df_b.to_csv('balanced.csv', index=False)

import re
import torch
import numpy as np
from torch.utils.data import Dataset, DataLoader, TensorDataset, random_split
from torch.nn.utils.rnn import pad_sequence
from tokenizers import Tokenizer, models, trainers, pre_tokenizers
from tokenizers.models import WordLevel
from tokenizers.trainers import WordLevelTrainer
from tokenizers.pre_tokenizers import Whitespace
from sklearn.preprocessing import LabelEncoder

# clean data
def clean_text(text):
    text = re.sub(r'\[.*?\]', '', text)
    text = text.replace('\n', ' ')
    text = ' '.join(text.split())
    text = text.lower()
    return text

df_b['lyrics'] = df_b['text'].apply(clean_text)

# Tokenizer
tokenizer = Tokenizer(WordLevel(unk_token="[UNK]"))

trainer = WordLevelTrainer(
    special_tokens=["[UNK]", "[CLS]", "[SEP]", "[PAD]", "[MASK]"],
    vocab_size=30000,
)

tokenizer.pre_tokenizer = Whitespace()
tokenizer.train_from_iterator(df_b['lyrics'], trainer=trainer)
vocab = tokenizer.get_vocab()

print(f"Vocab size: {len(vocab)}")
print(f"Total tokens in corpus: {sum(len(text.split()) for text in df_b['lyrics'])}")

# encode to token IDs
id_seq = [
    torch.tensor(x.ids, dtype=torch.int64)
    for x in tokenizer.encode_batch(df_b['lyrics'])
]

# pad sequences
padded_seq = pad_sequence(id_seq, batch_first=True)
padded_seq = padded_seq[:, :100]
print(f"Padded sequences shape: {padded_seq.shape}")

# pncode emotion labels
label_encoder = LabelEncoder()
df_b['emotion_encoded'] = label_encoder.fit_transform(df_b['emotion'])

# show label mapping
print("\nEmotion label mapping:")
for i, emotion in enumerate(label_encoder.classes_):
    count = (df_b['emotion'] == emotion).sum()
    print(f"  {emotion}: {i} (count: {count})")

# convert to tensor
targets = torch.tensor(df_b['emotion_encoded'].values, dtype=torch.int64)
print(f"Targets shape: {targets.shape}")
print(f"Targets dtype: {targets.dtype}")

# create dataset object
dataset = torch.utils.data.TensorDataset(padded_seq, targets)
print(f"Dataset created with {len(dataset)} samples")


torch.manual_seed(0)
(train_dataset, val_dataset) = random_split(dataset, [0.7, 0.3])
len(train_dataset), len(val_dataset)

vocab_size = len(vocab)
num_classes = len(df['emotion'].unique())

# CUSTOM LSTM MODEL

import torch
import torch.nn as nn
import torch.nn.functional as F
from lightning.pytorch import LightningModule
from torchmetrics import Accuracy, F1Score
from lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor
from torchmetrics.classification import BinaryAccuracy

# LIGHTNING MODULE
class MyLightning(LightningModule):
    def __init__(self, learning_rate=0.001, weight_decay=0.01):
        super().__init__()
        # self.save_hyperparameters()

        #torchmetrics accuracy
        self.accuracy = Accuracy(task="multiclass", num_classes=num_classes)

        #F1 Scores
        # self.train_f1 = F1Score(task="binary")
        # self.val_f1 = F1Score(task="binary")

    def training_step(self, batch, batch_idx):
        x, target = batch
        y = self.forward(x)
        loss = nn.CrossEntropyLoss()(y, target)

        # get predictions
        preds = torch.argmax(y, dim = 1)
        self.accuracy(preds, target)

        self.log("train_loss", loss, prog_bar=True)
        self.log("accuracy", self.accuracy, prog_bar=True)
        return loss

    def configure_optimizers(self):
        return torch.optim.Adam(self.parameters())

    def validation_step(self, batch, batch_idx):

        x, target = batch

        # forward pass
        y = self.forward(x)

        # make sure target is a tensor
        if isinstance(target, list):
            target = torch.tensor(target, device=y.device)

        # check if we have proper shape for binary classification
        if y.shape[1] > 1:
            preds = torch.argmax(y, dim=1)
        else:

            preds = (y > 0.5).float()

        # accuracy
        self.accuracy(preds, target)
        self.log("val_accuracy", self.accuracy, prog_bar=True)


class MyLSTM(MyLightning):
    def __init__(self, vocab_size=len(vocab), dim_emb=200, dim_hidden=256,
                 num_classes=num_classes, dropout=0.5, learning_rate=0.001, filename="metrics.txt"):
        super().__init__()
        self.vocab_size = vocab_size
        self.dim_emb = dim_emb
        self.dim_hidden = dim_hidden
        self.num_classes = num_classes
        self.dropout = dropout
        self.learning_rate = learning_rate

        #Embedding
        self.embedding = nn.Embedding(vocab_size, dim_emb, padding_idx=0)

        def init_weights(self):
          initrange = 0.1
          nn.init.uniform_(self.encoder.weight, -initrange, initrange)
          nn.init.zeros_(self.decoder.bias)
          nn.init.uniform_(self.decoder.weight, -initrange, initrange)

        # LSTM with layer normalization
        self.lstm = nn.LSTM(
            input_size=dim_emb,
            hidden_size=dim_hidden,
            num_layers=2,
            batch_first=True,
            #bidirectional LSTM
            bidirectional=True,
            dropout=dropout
        )
        self.output = nn.Linear(dim_hidden, num_classes)

        #attention
        self.attention = nn.Linear(dim_hidden * 2, 1, bias=False)

        #classifier
        self.classifier = nn.Sequential(
            nn.Dropout(dropout),
            nn.Linear(dim_hidden, 128),
            nn.ReLU(),
            nn.Dropout(dropout * 0.5),
            nn.Linear(128, num_classes)
        )

    def forward(self, batch_of_seqs):
      emb = self.embedding(batch_of_seqs)
      _, (state, _) = self.lstm(emb)
      # state: (num_layers, batch, dim_state)
      output = self.output(state[-1])
      return output

    def init_hidden(self, bsz):
        weight = next(self.parameters())
        return (
            weight.new_zeros(self.nlayers, bsz, self.nhid),
            weight.new_zeros(self.nlayers, bsz, self.nhid),
        )


from lightning.pytorch.callbacks import ModelCheckpoint
from lightning.pytorch.loggers import CSVLogger
from lightning.pytorch import Trainer
from lightning import seed_everything
import shutil, os, time


batch_size = 32
train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)




def train(*, name:str, model:LightningModule, epochs: int):
  seed_everything(0)

  logger = CSVLogger(save_dir="logs/", name=name)
  trainer = Trainer(
      max_epochs=epochs,
      logger=logger,
  )


  try:
    shutil.rmtree(f"./logs/{name}/")
    os.mkdirs(f"./logs/{name}")
  except:
    pass

  start = time.time()
  trainer.fit(model, train_dataloader, val_dataloader)\


  duration = time.time() - start
  print(f"Training time: {duration:0.2f} seconds.")

#CUSTOM MODEL
model = MyLSTM(
    vocab_size=len(vocab),
    dim_emb=200,
    dim_hidden=256,
    num_classes=3,
    dropout=0.5,
    learning_rate=0.0001,
    filename = "metrics_CustomLSTM.txt"
)

train(
    name="MyLSTM",
    model=model,
    epochs=15
)

class MyLSTM(MyLightning):
  def __init__(self, input_size, hidden_size):
    super().__init__()
    self.accuracy = Accuracy(task="multiclass", num_classes=num_classes)

    self.embedding = nn.Embedding(vocab_size, input_size)

    self.lstm = nn.LSTM(input_size=input_size,
                      hidden_size=hidden_size,
                      num_layers=1,
                      batch_first=True)

    self.output = nn.Linear(hidden_size, num_classes)

  def forward(self, batch_of_seqs):
    emb = self.embedding(batch_of_seqs)
    _, (state, _) = self.lstm(emb)
    output = self.output(state[-1])
    return output

train(name="lstm", model=MyLSTM(200, 256), epochs=8)

class MyRNN(MyLightning):
  def __init__(self, input_size, hidden_size,):
    super().__init__()
    #
    # build the network architecture
    #
    self.embedding = nn.Embedding(vocab_size, input_size)
    self.rnn = nn.RNN(input_size, hidden_size, num_layers=1, batch_first=True)
    self.fc = nn.Linear(hidden_size, num_classes)

    #
    # measure classification accuracy
    #
    self.accuracy = Accuracy(task="multiclass", num_classes=num_classes)

  def forward(self, batch_of_seqs):
    #
    # RNN performs reduce on the embedding sequence
    #
    embeddings = self.embedding(batch_of_seqs)
    out, _ = self.rnn(embeddings)
    out = self.fc(out[:, -1, :])
    return out

train(name="lstm", model=MyRNN(200, 256), epochs=8)

! pip install transformers

!pip install -U datasets evaluate transformers==4.49.0 sentencepiece huggingface_hub fsspec

#TRANSFORMER BERT MODEL

import os
import torch
from torch import nn
from torch.utils.data import DataLoader, Dataset
from transformers import BertTokenizer, BertModel, get_linear_schedule_with_warmup
from torch.optim import AdamW
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
import pandas as pd

class BERTClassifier(nn.Module):
    def __init__(self, bert_model_name, num_classes):
        super(BERTClassifier, self).__init__()
        self.bert = BertModel.from_pretrained(bert_model_name)
        self.dropout = nn.Dropout(0.1)
        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)

    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        pooled_output = outputs.pooler_output
        x = self.dropout(pooled_output)
        logits = self.fc(x)
        return logits


def train(model, data_loader, optimizer, scheduler, device):
    print("model.train")
    model.train()
    for batch in data_loader:
        print("for batch")
        optimizer.zero_grad()
        print("optimizer.zero_grad")
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        print("input_ids = batch['input_ids'].to(device)")
        labels = batch['label'].to(device)
        print("labels = batch['label'].to(device)")
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        print("loss")
        loss = nn.CrossEntropyLoss()(outputs, labels)
        print("loss.backward()")
        loss.backward()
        print("optimizer.step()")
        optimizer.step()
        print("scheduler.step()")
        scheduler.step()


def evaluate(model, data_loader, device):
    model.eval()
    predictions = []
    actual_labels = []
    with torch.no_grad():
        for batch in data_loader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['label'].to(device)
            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            _, preds = torch.max(outputs, dim=1)
            predictions.extend(preds.cpu().tolist())
            actual_labels.extend(labels.cpu().tolist())
    return accuracy_score(actual_labels, predictions), classification_report(actual_labels, predictions)

from google.colab import drive
drive.mount('/content/drive')

import shutil
import os

source_path = '900k Definitive Spotify Dataset.json'
destination_folder = '/content/drive/MyDrive/Colab_Data'
os.makedirs(destination_folder, exist_ok=True)

destination_path = os.path.join(destination_folder, os.path.basename(source_path))
shutil.copy(source_path, destination_path)
print(f"File '{source_path}' copied to '{destination_path}'")

def predict_sentiment(text, model, tokenizer, device, max_length=128):
    model.eval()
    encoding = tokenizer(text, return_tensors='pt', max_length=max_length, padding='max_length', truncation=True)
    input_ids = encoding['input_ids'].to(device)
    attention_mask = encoding['attention_mask'].to(device)

    with torch.no_grad():
            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            _, preds = torch.max(outputs, dim=1)
    return "positive" if preds.item() == 1 else "negative"

# most standard BERT model
bert_model_name = 'bert-base-uncased'
num_classes = len(df_b['emotion'].unique())
max_length = 128
batch_size = 32
num_epochs = 8
learning_rate = 0.001

class TextClassificationDataset(Dataset):
  def __init__(self, texts, labels, tokenizer, max_length):
          self.texts = texts
          self.labels = labels
          self.tokenizer = tokenizer
          self.max_length = max_length
  def __len__(self):
      return len(self.texts)
  def __getitem__(self, idx):
      text = self.texts[idx]
      label = self.labels[idx]
      encoding = self.tokenizer(text, return_tensors='pt', max_length=self.max_length, padding='max_length', truncation=True)
      return {'input_ids': encoding['input_ids'].flatten(), 'attention_mask': encoding['attention_mask'].flatten(), 'label': torch.tensor(label)}

texts = df_b['text'].tolist()
labels = df_b['emotion_encoded'].tolist()

train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)

tokenizer = BertTokenizer.from_pretrained(bert_model_name)
train_dataset = TextClassificationDataset(train_texts, train_labels, tokenizer, max_length)
val_dataset = TextClassificationDataset(val_texts, val_labels, tokenizer, max_length)


train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_dataloader = DataLoader(val_dataset, batch_size=batch_size)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = BERTClassifier(bert_model_name, num_classes).to(device)

def train(model, data_loader, optimizer, scheduler, device):
    model.train()
    for batch in data_loader:
        optimizer.zero_grad()
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['label'].to(device)
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        loss = nn.CrossEntropyLoss()(outputs, labels)
        loss.backward()
        optimizer.step()
        scheduler.step()

#TRAINING BERT MODEL


optimizer = AdamW(model.parameters(), lr=learning_rate)
total_steps = len(train_dataloader) * num_epochs
scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)

for epoch in range(num_epochs):
        print(f"Epoch {epoch + 1}/{num_epochs}")
        print("calling train")
        train(model, train_dataloader, optimizer, scheduler, device)
        print("calling eval")
        accuracy, report = evaluate(model, val_dataloader, device)
        print(f"Validation Accuracy: {accuracy:.4f}")
        print(report)

import matplotlib.pyplot as plt
import pandas as pd

perf_cus = pd.read_csv('custom.csv')
perf_lstm = pd.read_csv('lstm.csv')
perf_rnn = pd.read_csv('metrics.csv')

perf_bert = {0:0.66,
             10:0.69,
             20:0.71}


val_acc = pd.concat([perf_cus.val_accuracy.dropna(), perf_lstm.val_accuracy.dropna(), perf_rnn.val_accuracy.dropna()], axis=1)

bert_s = pd.Series(perf_bert)
bert_s.name = 'BERT'



fig = plt.figure()
ax = fig.gca()

ax.set_xlabel('Epoch')
ax.set_ylabel('Accuracy')

val_acc.columns = ['custom', 'lstm', 'rnn']
val_acc.plot(ax=ax)

ax.plot(bert_s.index, bert_s.values,
        label='BERT')


ax.legend(['Custom LSTM Model', 'LSTM', 'RNN', 'BERT'])
ax.set_xlim(0,60)
plt.show()

! pip install geniuslyrics





import torch
import torch.nn as nn
from lightning.pytorch import LightningModule
import pickle

class EmotionClassifierFromCheckpoint:
    def __init__(self, checkpoint_path, vocab_path=None, class_names=None, device=None):
        """
        Initialize classifier from Lightning checkpoint

        Args:
            checkpoint_path: Path to .ckpt file
            vocab_path: Path to saved vocabulary (if separate)
            class_names: List of emotion class names
            device: 'cuda' or 'cpu'
        """
        if device is None:
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        else:
            self.device = device

        # Load the model from checkpoint
        # Method 1: Load the entire Lightning module
        self.model = MyLSTM.load_from_checkpoint(checkpoint_path)
        self.model.to(self.device)
        self.model.eval()
        self.model.freeze()  # Important for inference

        # Extract hyperparameters from the model
        self.hparams = self.model.hparams if hasattr(self.model, 'hparams') else {}

        # Load vocabulary (you might have saved it separately)
        if vocab_path:
            with open(vocab_path, 'rb') as f:
                self.vocab = pickle.load(f)
        else:
            # Try to get vocab from model or checkpoint
            if hasattr(self.model, 'vocab'):
                self.vocab = self.model.vocab
            else:
                raise ValueError("Vocabulary not found. Provide vocab_path.")

        # Get class names
        if class_names:
            self.class_names = class_names
        elif hasattr(self.model, 'class_names'):
            self.class_names = self.model.class_names
        else:
            # If not stored, you'll need to know your classes
            # You might have this from your dataset
            self.class_names = ['happy', 'sad', 'angry', 'peaceful']  # Update with your actual classes

    def preprocess_text(self, text, max_length=100):
        """
        Preprocess text to match training format
        """
        # Use the same tokenization as during training
        tokens = text.lower().split()

        # Convert to indices
        token_indices = []
        for token in tokens:
            if token in self.vocab.stoi:  # Using torchtext vocab
                token_indices.append(self.vocab.stoi[token])
            else:
                token_indices.append(self.vocab.stoi['<unk>'])  # Use UNK token if available

        # Pad/truncate
        if len(token_indices) >= max_length:
            token_indices = token_indices[:max_length]
        else:
            token_indices = token_indices + [0] * (max_length - len(token_indices))

        return torch.tensor([token_indices], dtype=torch.long).to(self.device)

    def predict(self, text, return_probabilities=False):
        """
        Predict emotion for given lyrics
        """
        with torch.no_grad():
            input_tensor = self.preprocess_text(text)
            outputs = self.model(input_tensor)

            # For classification, use softmax
            probabilities = torch.softmax(outputs, dim=1)
            predicted_class = torch.argmax(probabilities, dim=1).item()

            if return_probabilities:
                prob_dict = {
                    self.class_names[i]: float(probabilities[0][i])
                    for i in range(len(self.class_names))
                }
                return prob_dict
            else:
                return self.class_names[predicted_class]

! pip install PySide6

